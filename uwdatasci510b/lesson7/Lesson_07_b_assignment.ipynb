{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "## Summary of instructions\n",
    "\n",
    "1. <u>Get Corpora:</u>  use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "<br/> <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "2. <u>Combine to Corpus:</u> concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "3. <u>Rename Labels:</u> Replace sentiment labels 'POLIT': 1, 'NOT': 0; <span style=\"color:red\" float:right>[0 point]</span>\n",
    "\n",
    "4. <u>Clean Tweets:</u>\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "<br/><span style=\"color:red\" float:right>[9 point]</span>\n",
    "\n",
    "5. <u>Create TF-IDF:</u> Use TfidfVectorizer from sklearn to prepare the data for machine learning.  Use max_features = 50;  <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "6. <u>Train Classifier:</u> Use sklearn LogisticRegression to train a model on the results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "7. <u>Determine Accuracies:</u>  Determine the accuracy on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "8. <u>Relate number of features to accuracy:</u> Repeat steps 5, 6, and 7  with TfidfVectorizer max_features set to 1, 2, 5, 15, 50, 150, 500, 1500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "# Basic Data Science packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Basic text processing packages\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Basic Machine Learning packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Data Presentation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <u>Get Corpora:</u>  use `pandas` `read_csv` with `sep='\\t'` to read in the following 2 files available from the us naval academy:\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "<br/> <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "Elaboration:  Read in the data directly from the files posted on the websites.  Alternately, first download the files and then read in the data from your local copies.  The result will be two dataframes with default column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "txtfiles = ['../../data/keyword-tweets.txt', '../../data/general-tweets.txt']\n",
    "names = ['orig_label','context']\n",
    "tweet_df = pd.read_csv(txtfiles[0],sep='\\t',names=names)\n",
    "other = pd.read_csv(txtfiles[1],sep='\\t',names=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <u>Combine to Corpus:</u> concatenate these 2 data sets into a single data frame called `LabeledTweets` that has 2 columns, named `Sentiment` and `Tweet` <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "Elaboration:  Concatenate the two data frames.  Watch out that the indices are reset and are sequential from 0 to 4003.  After concatenation, rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here\n",
    "tweet_df = tweet_df.append(other,sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. <u>Rename Labels:</u> Replace sentiment labels `'POLIT': 1, 'NOT': 0`; <span style=\"color:red\" float:right>[0 point]</span>\n",
    "\n",
    "Elaboration:  No need to cast the sentiment column to the category data type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_label</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orig_label                                            context  label\n",
       "0      POLIT  Global Voices Online Â» Alex Castro: A liberal...      1\n",
       "1      POLIT  Do the Conservatives Have a Death Wish? http:/...      1\n",
       "2        NOT  @MMFlint I've seen all of your movies and Capi...      0\n",
       "3      POLIT  RT @AllianceAlert: * House Dems ask for civili...      1\n",
       "4      POLIT  RT @AdamSmithInst Quote of the week: My politi...      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code here\n",
    "tweet_df['label'] = tweet_df['orig_label'].transform(lambda x:1 if x == 'POLIT' else 0)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <u>Clean Tweets:</u>\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because `TfidfVectorizer` will take care of that\n",
    "<br/><span style=\"color:red\" float:right>[9 point]</span>\n",
    "\n",
    "Elaboration:  This task goes beyond our lecture and you will need to look online to find examples on how to accomplish the above cleanings.  I would make heavy use of string methods like:\n",
    "- Does a string contain a substring?  Use `in`; example: `'@' in token`\n",
    "- Does a string contain a number?  Use `isdigit()`; example: `char.isdigit()`\n",
    "\n",
    "You can also use the regular expressions (regex) and its functions like `sub`.  Python's regular expression package is `re`.  Just like in class the cleaned tweets are added as a new column to `LabeledTweets`.  The new column could be called `clean_tweet`, like in class.  The presented order of the cleaning steps is probably not the best.  Establish your own order and justify your order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning Discussion\n",
    "Add Comment Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ayaskanti/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_label</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "      <td>1</td>\n",
       "      <td>global voice online alex castro a liberal libe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "      <td>1</td>\n",
       "      <td>do the conservative have a death wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "      <td>0</td>\n",
       "      <td>i ve seen all of your movie and capitalism is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt house dems ask for civility at town hall an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt quote of the week my political opinion lean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orig_label                                            context  label  \\\n",
       "0      POLIT  Global Voices Online Â» Alex Castro: A liberal...      1   \n",
       "1      POLIT  Do the Conservatives Have a Death Wish? http:/...      1   \n",
       "2        NOT  @MMFlint I've seen all of your movies and Capi...      0   \n",
       "3      POLIT  RT @AllianceAlert: * House Dems ask for civili...      1   \n",
       "4      POLIT  RT @AdamSmithInst Quote of the week: My politi...      1   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  global voice online alex castro a liberal libe...  \n",
       "1              do the conservative have a death wish  \n",
       "2  i ve seen all of your movie and capitalism is ...  \n",
       "3  rt house dems ask for civility at town hall an...  \n",
       "4  rt quote of the week my political opinion lean...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import itertools\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Add code here\n",
    "def preprocess(text:str) -> str:\n",
    "    # tokenize\n",
    "    text = list(text.split())\n",
    "    \n",
    "    # A B\n",
    "    text = filter(lambda x:'@' not in x and 'http' not in x, text)\n",
    "    text= ' '.join(text)\n",
    "    \n",
    "    # C D E\n",
    "    translateToSpace = {x for x in itertools.chain(string.punctuation,'0123456789','\\t')}\n",
    "    text = map(lambda x: ' ' if x in translateToSpace or ord(x) >= 128 else x.lower(), text)\n",
    "    \n",
    "    # tokenize\n",
    "    text = ''.join(text)\n",
    "\n",
    "    # G H\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "\n",
    "tweet_df['clean_tweet'] = tweet_df['context'].map(preprocess)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. <u>Create TF-IDF:</u> Use `TfidfVectorizer` from `sklearn` to prepare the data for machine learning.  Use `max_features = 50000`;  <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "Elaboration:  You can copy the example in lecture.  I would play with max_df, min_df, and sublinear_tf.  `fit_transform` is applied to the cleaned Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_label</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "      <td>1</td>\n",
       "      <td>global voice online alex castro a liberal libe...</td>\n",
       "      <td>(0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "      <td>1</td>\n",
       "      <td>do the conservative have a death wish</td>\n",
       "      <td>(0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "      <td>0</td>\n",
       "      <td>i ve seen all of your movie and capitalism is ...</td>\n",
       "      <td>(0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt house dems ask for civility at town hall an...</td>\n",
       "      <td>(0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt quote of the week my political opinion lean...</td>\n",
       "      <td>(0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orig_label                                            context  label  \\\n",
       "0      POLIT  Global Voices Online Â» Alex Castro: A liberal...      1   \n",
       "1      POLIT  Do the Conservatives Have a Death Wish? http:/...      1   \n",
       "2        NOT  @MMFlint I've seen all of your movies and Capi...      0   \n",
       "3      POLIT  RT @AllianceAlert: * House Dems ask for civili...      1   \n",
       "4      POLIT  RT @AdamSmithInst Quote of the week: My politi...      1   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  global voice online alex castro a liberal libe...   \n",
       "1              do the conservative have a death wish   \n",
       "2  i ve seen all of your movie and capitalism is ...   \n",
       "3  rt house dems ask for civility at town hall an...   \n",
       "4  rt quote of the week my political opinion lean...   \n",
       "\n",
       "                                               tfidf  \n",
       "0    (0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...  \n",
       "1    (0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...  \n",
       "2    (0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...  \n",
       "3    (0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...  \n",
       "4    (0, 3705)\\t0.2742019638001817\\n  (0, 9685)\\t...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code here\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 50000, stop_words = 'english')\n",
    "clean_texts = tweet_df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. <u>Train Classifier:</u> Use sklearn LogisticRegression to train a model on the results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "Elaborate:  Just like in lecture use `train_test_split` from `sklearn` to split the vectorized features qlong with the target, `LabeledTweets.Sentiment`.  Fit a logistic regression using the resulting training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (3204, 10209)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayaskanti/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, tweet_df['label'], test_size = 800, random_state = 42)\n",
    "print(type(X_train), X_train.shape)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. <u>Determine Accuracies:</u>  Determine the accuracy on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "Elaboration:  The only accuracy measure required is accuracy rate.  Accuracy rate is determined by correct predictions divided by all predictions.  In lecture these were called Train accuracy, Test accuracy, and Baseline accuracy.  The Baseline accuracy can be determinbed by taking the support of the larger class and dividing it by the total number of cases (tweets).  As we will discuss in a later lecture, only Test accuracy is a \"real\" accuracy.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.954119850187266\n",
      "Test accuracy: 0.865\n",
      "Baseline accuracy: 0.54125\n"
     ]
    }
   ],
   "source": [
    "# Add code here\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. <u>Relate number of features to accuracy:</u> Repeat steps 5, 6, and 7  with `TfidfVectorizer` `max_features` set to 1, 2, 5, 15, 50, 150, 500, 1500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "Elaboration:  Create a for-loop where `max_features` is 1, 2, 5, 15, 50, 150, 500, 1500, 5000, 50000.  Plot the training and test accuracies vs number of features on a log plot.  Discuss the behavior of the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayaskanti/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/lib/python3/dist-packages/seaborn/categorical.py:1508: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
      "  stat_data = remove_na(group_data[hue_mask])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff7e1c6d4e0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sea\n",
    "from operator import itemgetter\n",
    "\n",
    "# Add code here\n",
    "def modelMaker(max_features):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = max_features, stop_words = 'english')\n",
    "    clean_texts = tweet_df['clean_tweet']\n",
    "    tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "    #tweet_df.head()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, tweet_df['label'], test_size = 800, random_state = 42)\n",
    "    #print(type(X_train), X_train.shape)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    train_results = lr.predict(X_train)\n",
    "    test_results = lr.predict(X_test)\n",
    "\n",
    "    train_acc = np.mean(y_train == train_results)\n",
    "    test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "    return (train_acc, test_acc )#, np.max([np.mean(y_test == 1), np.mean(y_test == 0)])]\n",
    "\n",
    "featList = [1, 2, 5, 15, 50, 150, 500, 1500, 5000, 50000]\n",
    "trainTest = [modelMaker(x) for x in featList]\n",
    "trainList= list(map(itemgetter(0),trainTest))\n",
    "testList= list(map(itemgetter(1),trainTest))\n",
    "\n",
    "trainData= pd.DataFrame(data={'feat':featList,'acc':trainList,'type':'train'})\n",
    "testData= pd.DataFrame(data={'feat':featList,'acc':testList,'type':'test'})\n",
    "\n",
    "pltdata = testData.append(trainData)\n",
    "pltdata.head()\n",
    "sea.barplot(data=pltdata,x='feat',y='acc',hue='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss accuracies in respect to number of features\n",
    "When we used more feature , the accuracy on training set increased.\n",
    "\n",
    "But unlike training set, by using features upto 150 we got best test-accuracy and after that it declined.\n",
    "\n",
    "This means that more feature does not always give better model, after certain point it plateau.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
